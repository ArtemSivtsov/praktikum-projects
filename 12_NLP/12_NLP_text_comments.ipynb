{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Проект 12. Обработка текстов (NLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Интернет-магазин «Викишоп» запускает новый сервис. Теперь пользователи могут редактировать и дополнять описания товаров, как в вики-сообществах. То есть клиенты предлагают свои правки и комментируют изменения других. Магазину нужен инструмент, который будет искать токсичные комментарии и отправлять их на модерацию. \n",
    "\n",
    "Обучите модель классифицировать комментарии на позитивные и негативные. В вашем распоряжении набор данных с разметкой о токсичности правок.\n",
    "\n",
    "Постройте модель со значением метрики качества *F1* не меньше 0.7. \n",
    "\n",
    "### Инструкция по выполнению проекта\n",
    "\n",
    "1. Загрузите и подготовьте данные.\n",
    "2. Обучите разные модели. \n",
    "3. Сделайте выводы.\n",
    "\n",
    "Для выполнения проекта применять *BERT* необязательно, но вы можете попробовать.\n",
    "\n",
    "### Описание данных\n",
    "\n",
    "Данные находятся в файле `toxic_comments.csv`. Столбец *text* в нём содержит текст комментария, а *toxic* — целевой признак."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Подготовка"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"> <b>Комментарий студента:</b> Импортируем библиотеки </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "pd.set_option('display.max_columns', None)\n",
    "#pd.set_option('display.max_rows', None)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import re\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/datasets/toxic_comments.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-39f7127a4d60>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/datasets/toxic_comments.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    684\u001b[0m     )\n\u001b[1;32m    685\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 452\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    944\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    945\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 946\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1176\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1178\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1179\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   2006\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2007\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2008\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2009\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2010\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/datasets/toxic_comments.csv'"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('/datasets/toxic_comments.csv').head(50000)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50000 entries, 0 to 49999\n",
      "Data columns (total 2 columns):\n",
      "text     50000 non-null object\n",
      "toxic    50000 non-null int64\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 781.4+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"> <b>Комментарий студента:</b> Лемматизируем наш текст комментариев, попутно удаляя лишние символы </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Исходный текст:\n",
      " Explanation\n",
      "Why the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remove the template from the talk page since I'm retired now.89.205.38.27\n",
      "Очищенный и лемматизированный текст:\n",
      " Explanation Why the edits make under my username Hardcore Metallica Fan be revert They weren t vandalism just closure on some GAs after I vote at New York Dolls FAC And please don t remove the template from the talk page since I m retire now\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Lemmatize with POS Tag\n",
    "from nltk.corpus import wordnet\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "corpus = df['text'] #list(df['text'])   !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! 0-100!!!!!\n",
    "\n",
    "def lemmatize(text):\n",
    "    m = WordNetLemmatizer()\n",
    "    word_list = nltk.word_tokenize(text)\n",
    "    \n",
    "    # Lemmatize list of words and join\n",
    "    lemmatized_output = ' '.join([m.lemmatize(w, get_wordnet_pos(w)) for w in word_list])\n",
    "    \n",
    "    return lemmatized_output\n",
    "\n",
    "def clear_text(text):\n",
    "    x = re.sub(r'[^a-zA-Z]', ' ', text) \n",
    "    x = x.split()\n",
    "    return \" \".join(x)\n",
    "\n",
    "print(\"Исходный текст:\\n\", corpus[0])\n",
    "print(\"Очищенный и лемматизированный текст:\\n\", lemmatize(clear_text(corpus[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9min 15s, sys: 48.6 s, total: 10min 3s\n",
      "Wall time: 10min 9s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>lemm_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>49995</td>\n",
       "      <td>\"\\nWell, she's done writing articles. But does...</td>\n",
       "      <td>0</td>\n",
       "      <td>Well she s do write article But doesn t the UN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49996</td>\n",
       "      <td>There was also an Atlantis wrap back in 2001. ...</td>\n",
       "      <td>0</td>\n",
       "      <td>There be also an Atlantis wrap back in Maybe t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49997</td>\n",
       "      <td>\"\\nI've removed the various \"\"references\"\" as ...</td>\n",
       "      <td>0</td>\n",
       "      <td>I ve remove the various reference a a userpage...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49998</td>\n",
       "      <td>RE: Removal of Templates \\n\\nI would be most g...</td>\n",
       "      <td>1</td>\n",
       "      <td>RE Removal of Templates I would be most gratef...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49999</td>\n",
       "      <td>Exactly. I want the drawing to have come first...</td>\n",
       "      <td>0</td>\n",
       "      <td>Exactly I want the draw to have come first too...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  toxic  \\\n",
       "49995  \"\\nWell, she's done writing articles. But does...      0   \n",
       "49996  There was also an Atlantis wrap back in 2001. ...      0   \n",
       "49997  \"\\nI've removed the various \"\"references\"\" as ...      0   \n",
       "49998  RE: Removal of Templates \\n\\nI would be most g...      1   \n",
       "49999  Exactly. I want the drawing to have come first...      0   \n",
       "\n",
       "                                               lemm_text  \n",
       "49995  Well she s do write article But doesn t the UN...  \n",
       "49996  There be also an Atlantis wrap back in Maybe t...  \n",
       "49997  I ve remove the various reference a a userpage...  \n",
       "49998  RE Removal of Templates I would be most gratef...  \n",
       "49999  Exactly I want the draw to have come first too...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "df['lemm_text'] = corpus.apply(lambda x: lemmatize(clear_text(x)))\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"> <b>Комментарий студента:</b> создан столбец с леммами по нашему тексту комментариев </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"> <b>Комментарий студента:</b> выделим трейн и тест наборы, а также фичи и таргеты </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Обучающая выборка:\n",
      "(30000, 3)\n",
      "-----\n",
      "Обучающая выборка:\n",
      "(10000, 3)\n",
      "-----\n",
      "Тестовая выборка:\n",
      "(10000, 3)\n"
     ]
    }
   ],
   "source": [
    "train, valid_plus_test = train_test_split(df, test_size=0.4, random_state=123) #[0:1000]\n",
    "valid, test = train_test_split(valid_plus_test, test_size=0.5, random_state=123)\n",
    "\n",
    "print('Обучающая выборка:')\n",
    "print(train.shape)\n",
    "\n",
    "print('-----')\n",
    "print('Обучающая выборка:')\n",
    "print(valid.shape)\n",
    "\n",
    "print('-----')\n",
    "print('Тестовая выборка:')\n",
    "print(test.shape)\n",
    "\n",
    "features_train = train.drop(['text', 'toxic'], axis=1)\n",
    "target_train = train['toxic'] # np.array(train['toxic'])\n",
    "\n",
    "features_valid = valid.drop(['text', 'toxic'], axis=1)\n",
    "target_valid = valid['toxic'] # np.array(test['toxic'])\n",
    "\n",
    "features_test = test.drop(['text', 'toxic'], axis=1)\n",
    "target_test = test['toxic'] # np.array(test['toxic'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44845\n",
      "5155\n",
      "8.699321047526674\n"
     ]
    }
   ],
   "source": [
    "print(df[df['toxic'] == 0]['toxic'].count())\n",
    "print(df[df['toxic'] == 1]['toxic'].count())\n",
    "\n",
    "print(df[df['toxic'] == 0]['toxic'].count() / df[df['toxic'] == 1]['toxic'].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "def upsample(features, target, repeat):\n",
    "    features_zeros = features[target == 0]\n",
    "    features_ones = features[target == 1]\n",
    "    target_zeros = target[target == 0]\n",
    "    target_ones = target[target == 1]\n",
    "\n",
    "    features_upsampled = pd.concat([features_zeros] + [features_ones] * repeat)\n",
    "    target_upsampled = pd.concat([target_zeros] + [target_ones] * repeat)\n",
    "    \n",
    "    features_upsampled, target_upsampled = shuffle(\n",
    "        features_upsampled, target_upsampled, random_state=12345)\n",
    "    \n",
    "    return features_upsampled, target_upsampled\n",
    "\n",
    "features_train_upsampled, target_train_upsampled = upsample(features_train, target_train, 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30000, 1)\n",
      "(30000,)\n",
      "\n",
      " upsampled\n",
      "(55104, 1)\n",
      "(55104,)\n",
      "\n",
      "(10000, 1)\n",
      "(10000,)\n",
      "\n",
      "(10000, 1)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "print(features_train.shape)\n",
    "print(target_train.shape)\n",
    "print('\\n upsampled')\n",
    "print(features_train_upsampled.shape)\n",
    "print(target_train_upsampled.shape)\n",
    "print()\n",
    "print(features_valid.shape)\n",
    "print(target_valid.shape)\n",
    "print()\n",
    "print(features_test.shape)\n",
    "print(target_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"> <b>Комментарий студента:</b> Создадим матрицу признаков со значениями TF-IDF </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus_train: (55104,)\n",
      "Размер матрицы train: (55104, 58318)\n",
      "\n",
      "corpus_valid: (10000,)\n",
      "Размер матрицы valid: (10000, 58318)\n",
      "\n",
      "corpus_test: (10000,)\n",
      "Размер матрицы test: (10000, 58318)\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "corpus_train = features_train_upsampled['lemm_text'].values.astype('U')\n",
    "corpus_valid = features_valid['lemm_text'].values.astype('U')\n",
    "corpus_test = features_test['lemm_text'].values.astype('U')\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stopwords = set(stopwords.words('english'))\n",
    "\n",
    "count_tf_idf = TfidfVectorizer(stop_words=stopwords)\n",
    "\n",
    "tf_idf_train = count_tf_idf.fit(corpus_train)\n",
    "tf_idf_train = count_tf_idf.transform(corpus_train)\n",
    "tf_idf_valid = count_tf_idf.transform(corpus_valid)\n",
    "tf_idf_test = count_tf_idf.transform(corpus_test)\n",
    "\n",
    "print('corpus_train:', corpus_train.shape)\n",
    "print(\"Размер матрицы train:\", tf_idf_train.shape) \n",
    "print()\n",
    "print('corpus_valid:', corpus_valid.shape)\n",
    "print(\"Размер матрицы valid:\", tf_idf_valid.shape)\n",
    "print()\n",
    "print('corpus_test:', corpus_test.shape)\n",
    "print(\"Размер матрицы test:\", tf_idf_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Обучение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"> <b>Комментарий студента:</b> Обучим несколько моделей (логистическая регрессия, дерево решений, случайный лес, sgd classifier) </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7214285714285715\n",
      "CPU times: user 5.23 s, sys: 4.37 s, total: 9.6 s\n",
      "Wall time: 9.62 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model_log = LogisticRegression(class_weight='balanced')\n",
    "model_log.fit(tf_idf_train, target_train_upsampled)\n",
    "predicted_log = model_log.predict(tf_idf_valid)\n",
    "\n",
    "print(f1_score(target_valid, predicted_log))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7140238313473877\n"
     ]
    }
   ],
   "source": [
    "predicted_log = model_log.predict(tf_idf_test)\n",
    "\n",
    "print(f1_score(target_test, predicted_log))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6074140241179098\n",
      "CPU times: user 17.4 s, sys: 13.8 ms, total: 17.4 s\n",
      "Wall time: 17.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model_tree = DecisionTreeClassifier(random_state=12345)\n",
    "model_tree.fit(tf_idf_train, target_train_upsampled)\n",
    "predicted_tree = model_tree.predict(tf_idf_valid)\n",
    "\n",
    "print(f1_score(target_valid, predicted_tree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5910313901345292\n"
     ]
    }
   ],
   "source": [
    "predicted_tree = model_tree.predict(tf_idf_test)\n",
    "\n",
    "print(f1_score(target_test, predicted_tree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model depth = 61\n",
      "F1_score = 0.6194774346793349\n",
      "CPU times: user 3min 14s, sys: 143 ms, total: 3min 14s\n",
      "Wall time: 3min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "best_model = None \n",
    "best_result = 0\n",
    "best_depth = 0\n",
    "for depth in range(1, 100, 5):\n",
    "    model_tree = DecisionTreeClassifier(random_state=12345, max_depth=depth)\n",
    "    model_tree.fit(tf_idf_train, target_train_upsampled)\n",
    "    predicted_tree = model_tree.predict(tf_idf_valid) \n",
    "    result_tree = f1_score(target_valid, predicted_tree) \n",
    "    if result_tree > best_result:\n",
    "        best_model = model_tree\n",
    "        best_result = result_tree\n",
    "        best_depth = depth\n",
    "        f1_tree = f1_score(target_valid, predicted_tree)\n",
    "\n",
    "print('best model depth =', best_depth)\n",
    "print(\"F1_score =\", f1_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5896568390526824\n"
     ]
    }
   ],
   "source": [
    "predicted_tree = best_model.predict(tf_idf_test)\n",
    "\n",
    "print(f1_score(target_test, predicted_tree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5835509138381201\n",
      "CPU times: user 11.2 s, sys: 18 ms, total: 11.2 s\n",
      "Wall time: 11.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model_for = RandomForestClassifier()\n",
    "model_for.fit(tf_idf_train, target_train_upsampled)\n",
    "predicted_for = model_for.predict(tf_idf_valid)\n",
    "\n",
    "print(f1_score(target_valid, predicted_for))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6051417270929466\n"
     ]
    }
   ],
   "source": [
    "predicted_for = model_for.predict(tf_idf_test)\n",
    "\n",
    "print(f1_score(target_test, predicted_for))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7084967320261438\n",
      "CPU times: user 250 ms, sys: 84.8 ms, total: 335 ms\n",
      "Wall time: 347 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model_s = SGDClassifier(class_weight='balanced')\n",
    "model_s.fit(tf_idf_train, target_train_upsampled)\n",
    "predicted_s = model_s.predict(tf_idf_valid)\n",
    "\n",
    "print(f1_score(target_valid, predicted_s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.706989247311828\n"
     ]
    }
   ],
   "source": [
    "predicted_s = model_s.predict(tf_idf_test)\n",
    "\n",
    "print(f1_score(target_test, predicted_s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Выводы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"> <b>Комментарий студента:</b> выполнены все задачи, поставленные в ходе выполнения проекта </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Чек-лист проверки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [x]  Jupyter Notebook открыт\n",
    "- [x]  Весь код выполняется без ошибок\n",
    "- [x]  Ячейки с кодом расположены в порядке исполнения\n",
    "- [x]  Данные загружены и подготовлены\n",
    "- [x]  Модели обучены\n",
    "- [x]  Значение метрики *F1* не меньше 0.7\n",
    "- [x]  Выводы написаны"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
